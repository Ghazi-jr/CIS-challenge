{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TBYkH4DvU4SM",
        "T9jBdP2CR7kf"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title TSYP CHALLENGE\n",
        "from IPython.display import HTML\n",
        "html_code = \"\"\"\n",
        "<div style=\"text-align: center;\">\n",
        "    <font color=\"#ffd700\" size=\"7\">\n",
        "        IEEE CS Tunisia in collaboration with IEEE YP Tunisia\n",
        "        <br>\n",
        "        Deep-fake Forensics Challenge\n",
        "    </font>\n",
        "</div>\n",
        "<br>\n",
        "<div style=\"text-align: center;\">\n",
        "    <font size=\"3\">\n",
        "        The challenge revolves around reproducing and enhancing the results presented in a research paper\n",
        "        entitled:\n",
        "        \"Shallow- and Deep- fake Image Manipulation Localization Using Deep Learning\" by Junbin Zhang,\n",
        "        Hamidreza Tohidypour, Yixiao Wang, and Panos Nasiopoulos.\n",
        "    </font>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "cellView": "form",
        "id": "09M6lHKEL6Cp",
        "outputId": "8082a683-519f-43dc-9aee-cc1b1b9425e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"text-align: center;\">\n",
              "    <font color=\"#ffd700\" size=\"7\">\n",
              "        IEEE CS Tunisia in collaboration with IEEE YP Tunisia\n",
              "        <br>\n",
              "        Deep-fake Forensics Challenge\n",
              "    </font>\n",
              "</div>\n",
              "<br>\n",
              "<div style=\"text-align: center;\">\n",
              "    <font size=\"3\">\n",
              "        The challenge revolves around reproducing and enhancing the results presented in a research paper\n",
              "        entitled:\n",
              "        \"Shallow- and Deep- fake Image Manipulation Localization Using Deep Learning\" by Junbin Zhang,\n",
              "        Hamidreza Tohidypour, Yixiao Wang, and Panos Nasiopoulos.\n",
              "    </font>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This report will reflect the notebook in the repository which addresses the reproducibility of the research work presented in the **Paper**:\n",
        "`\"Shallow- and Deep-fake Image Manipulation Localization Using Deep Learning\" by Junbin Zhang, Hamidreza Tohidypour, Yixiao Wang, and Panos Nasiopoulos.`\n",
        "\n",
        "## `Introduction`\n",
        "This report focuses on the critical task of reproducing the research outlined in the paper, exploring the significance of forged image localization and its impact on various aspects of society. The research specifically addresses both \"shallowfakes\" created with image editing tools and \"deepfakes\" generated using artificial intelligence techniques. The paper introduces a novel solution capable of effectively localizing manipulated areas in both shallow- and deep-fake images, achieving high inference accuracy.\n",
        "\n",
        "***The report will be outlined as following :***\n",
        "\n",
        "## `Datasets, Packages and Libraries Importing`\n",
        "- Import necessary Python packages and libraries.\n",
        "- Download The necessary datasets to reproduce the author's work\n",
        "\n",
        "## `Code Inspection and Performance Assessment`\n",
        "- Inspect and explain code implementation.\n",
        "- Evaluate the code from different perspectives, including its ease of implementation, performance characteristics, and its extensibility for future development.\n",
        "\n",
        "## `Experiments (Author's Work)`\n",
        "- Conduct the Authour's experiments to validate the proposed solution.\n",
        "- Apply the solution to various datasets for localization in manipulated areas.\n",
        "- Present results, visualizations, and comparisons with existing methods.\n",
        "\n",
        "## `Optimization (Ours)`\n",
        "- Explore opportunities for solution optimization.\n",
        "- Apply various optimization techniques to achieve better results.\n",
        "\n",
        "\n",
        "***In this report, sometimes will include code snippets so we could highlight how we reporduced the author's work and how we optimized on them, with this we will ensure that overall the report will summarize and well present the work we have done.***\n"
      ],
      "metadata": {
        "id": "rABSugNBMomp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = \"#ffd700\" size = 6> Datasets, Packages and Libraries Importing\n",
        "First, we wanted to ensure that we have imported the required Python packages and libraries. Subsequently, we download the essential datasets needed to replicate the author's research."
      ],
      "metadata": {
        "id": "IznGmjkWvzi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Packages and Libraries Importing`"
      ],
      "metadata": {
        "id": "TBYkH4DvU4SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown --quiet\n",
        "!pip install ninja --quiet"
      ],
      "metadata": {
        "id": "4FUC09LSDSuh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "17b71e7f-7ed4-4af3-ffc1-5c749b690078"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m225.3/307.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZhhV6btPukA-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from PIL import Image\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Dataset Downloading`\n",
        "\n"
      ],
      "metadata": {
        "id": "KDsZoPkPv4w4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`The dataset consists of two parts, one with shallowfake\n",
        "images and one with deepfake images.`\n",
        " - **Shallowfakes** : our training dataset is from **CASIAv2**. This dataset includes 7,490 authentic images and 4,948 forged images with slicing and copy-move manipulations, covering different kinds of objects. We split the dataset into training, validation, and test sets at a ratio of 8:1:1. That is to say, at the end we use 5,992 authentic images and 3,958 forged images in training our network. For testing, besides the above-mentioned 10% images from **CASIAv2**, we also included all images from **CASIAv1**, **Columbia**, **COVERAGE**, and **NIST16** in our test set to evaluate the generalizability of our model. In the end, our shallowfake test set includes 1,832 real and 2,259 fake images, covering all three types of shallowfake manipulations (i.e.,\n",
        "slicing, copy-move, and inpainting).\n",
        "\n",
        " - ***DeepFake*** : To address the absence of an image dataset with ground truth masks for manipulated areas in deepfakes, the authors created a dataset using FaceForensics++. This renowned dataset stands out as the sole source offering masks for a significant portion of its deepfake videos. FaceForensics++ comprises 1,363 genuine videos sourced from YouTube, 3,068 videos from Google's DeepFakeDetection dataset, and 5,000 fake videos generated through five distinct automated face manipulation techniques, namely Deepfakes, Face2Face, FaceShifter, FaceSwap, and NeuralTextures.\n",
        " <br>\n",
        " ---\n",
        " ***At this Section we will provide the work we did to download the diffrent datasets. We created some helper functions at first to achieve that. The detailled code will be provided in the notebook, we want to make this report contains less code as much as possible.***"
      ],
      "metadata": {
        "id": "DkwcaKYOEc-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helpers\n",
        "```\n",
        "Those functions will help us to download the different datasets employed by the authors\n",
        "```"
      ],
      "metadata": {
        "id": "T9jBdP2CR7kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_drive_dataset(link, dataset_name, extention = \"zip\") :\n",
        "  \"\"\"\n",
        "  Description :\n",
        "  -------------\n",
        "    This function will help us to download any dataset from google drive.\n",
        "\n",
        "  Params:\n",
        "  ------\n",
        "    - link : the google drive link of the dataset.\n",
        "    - dataset_name : the dataset name of the downloaded file and the output folder.\n",
        "    - extention : Compression Foramt\n",
        "\n",
        "  \"\"\"\n",
        "  file_id = link[link.find(\"/d/\") + 1:].strip('/view').strip(\"d/\")\n",
        "  file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_id\n",
        "\n",
        "  local_path = \"/content/\" + dataset_name + f\".{extention}\"\n",
        "\n",
        "  !gdown \"$file_download_link\" -O \"$local_path\"\n",
        "\n",
        "  if extention == \"zip\" :\n",
        "    path = f\"/content/{dataset_name}\"\n",
        "    if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "      print(f\"Dataset {dataset_name} directort is created !\")\n",
        "\n",
        "    print(\"Unzipping content...\")\n",
        "    !unzip -qq \"$local_path\" -d \"$path\"\n",
        "  else :\n",
        "    print(\"Decompressing method not found !\")"
      ],
      "metadata": {
        "id": "xadByHsiDyaz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset_dropbox(link, out_name, out_path) :\n",
        "  \"\"\"\n",
        "  Description :\n",
        "  -------------\n",
        "    This function will help us to download any dataset from dropbox.\n",
        "\n",
        "  Params:\n",
        "  ------\n",
        "    - link : the google drive link of the dataset.\n",
        "    - out_name : the dataset name of the downloaded file.\n",
        "    - out_path : Folder name which will contain our images after extraction.\n",
        "\n",
        "  \"\"\"\n",
        "  !wget -O  \"$out_name\" \"$link\"\n",
        "\n",
        "  path = f\"/content/{out_path}\"\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "    print(f\"Dataset {out_path} directort is created !\")\n",
        "\n",
        "  print(\"Unzipping content...\")\n",
        "  if \"zip\" in out_name :\n",
        "    !unzip -qq \"$out_name\" -d \"$path\"\n",
        "  else :\n",
        "    !tar -xf \"/content/$out_name\" -C \"$path\""
      ],
      "metadata": {
        "id": "xnGLLiK_QTrG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepFake Data\n"
      ],
      "metadata": {
        "id": "Emi5wvp3v71X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "link = \"https://www.dropbox.com/s/o5410tl5v4vxsth/ICNC2023-Deepfakes.tar.xz?dl=0\"\n",
        "out_name = \"deepfake.tar.xz\"\n",
        "out_path = \"deep-data\"\n",
        "\n",
        "download_dataset_dropbox(link, out_name, out_path)"
      ],
      "metadata": {
        "id": "Cb20xqTguoeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ShallowFake Data\n",
        "\n",
        "```\n",
        "To train and validate the shallowFake detection model, CASIAv2 will be employed by the authors.\n",
        "Subsequently, other datasets will be used for testing to evaluate the model's generalization capabilities.\n",
        "```"
      ],
      "metadata": {
        "id": "v8e9NWYdv-d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CASIAv2\n"
      ],
      "metadata": {
        "id": "rtnHIsRnwD0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "link = \"https://drive.google.com/file/d/1IDUgcoUeonBxx2rASX-_QwV9fhbtqdY8/view\"\n",
        "dataset_name = \"casiav2\"\n",
        "download_drive_dataset(link, dataset_name)"
      ],
      "metadata": {
        "id": "a_BL-jOHCAjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***After downloading the CASIAv2 dataset we need to Clone the Masks from CasiaV2 Repo and copy it to the extracted dataset***"
      ],
      "metadata": {
        "id": "ZyURjWDdyUyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/namtpham/casia2groundtruth\n",
        "!unzip -qq \"/content/casia2groundtruth/CASIA2.0_Groundtruth.zip\" -d \"/content/casiav2/CASIA2.0_revised/\""
      ],
      "metadata": {
        "id": "V77m03mxyUNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CASIAv1"
      ],
      "metadata": {
        "id": "X8v7TKLmcVv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link = \"https://drive.google.com/file/d/14f3jU2VsxTYopgSE1Vvv4hMlFvpAKHUY/view\"\n",
        "dataset_name = \"casiav1\"\n",
        "download_drive_dataset(link, dataset_name)"
      ],
      "metadata": {
        "id": "kF7qaSo9b9JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We followed the same approach here for the CASIAv2 dataset, we need to clone the repo to retrieve the ground truth masks***"
      ],
      "metadata": {
        "id": "7d8UqauJj0xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/namtpham/casia1groundtruth\n",
        "!unzip -qq \"/content/casia1groundtruth/CASIA 1.0 groundtruth.zip\" -d \"/content/casiav1/CASIA 1.0 dataset/\""
      ],
      "metadata": {
        "id": "NkaovRd6jyZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***For this specific Dataset we needed extra steps to follow so we could have our dataset ready. So we extracted the content inside the unzipped downloaded dataset to obtain the Authentic and modified images. We also renamed the datasets folder because spaces will yield to some problems when we try to parse the paths and read our images in training later***"
      ],
      "metadata": {
        "id": "WABoSxYS08_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/casiav1/CASIA\\ 1.0\\ dataset/ /content/casiav1/CASIA_1.0_dataset/\n",
        "!mv /content/casiav1/CASIA_1.0_dataset/CASIA\\ 1.0\\ groundtruth/ /content/casiav1/CASIA_1.0_dataset/CASIA_1.0_groundtruth/"
      ],
      "metadata": {
        "id": "AK1w8-ibBHJ4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq /content/casiav1/CASIA_1.0_dataset/Au.zip -d /content/casiav1/CASIA_1.0_dataset/\n",
        "!unzip -qq /content/casiav1/CASIA_1.0_dataset/Modified\\ Tp.zip -d /content/casiav1/CASIA_1.0_dataset/"
      ],
      "metadata": {
        "id": "aRvL82jVk3oW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Columbia"
      ],
      "metadata": {
        "id": "SNRi_45Mclmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***At this step we will download the Columbia Dataset, we will extract the content and we will change the folder layouts of our datasets so we would prevent training problems later***"
      ],
      "metadata": {
        "id": "egbLB68K1bRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "link = \"https://www.dropbox.com/sh/786qv3yhvc7s9ki/AACbEEzGPrD3_y38bpWHzgdqa?dl=0\"\n",
        "out_name = \"colombia.zip\"\n",
        "out_path = \"colombia\"\n",
        "\n",
        "download_dataset_dropbox(link, out_name, out_path)"
      ],
      "metadata": {
        "id": "nH16On2mfUH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/colombia/4cam_auth\n",
        "!rm -rf /content/colombia/4cam_splc"
      ],
      "metadata": {
        "id": "cz36q466CZf1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf \"/content/colombia/4cam_auth.tar.bz2\" -C \"/content/colombia\"\n",
        "!tar -xf \"/content/colombia/4cam_splc.tar.bz2\" -C \"/content/colombia\""
      ],
      "metadata": {
        "id": "nPeXVr3wtRNA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/colombia/masks\n",
        "!cp -r /content/colombia/4cam_auth/edgemask /content/colombia/masks\n",
        "!cp -a /content/colombia/4cam_splc/edgemask/. /content/colombia/masks/edgemask/"
      ],
      "metadata": {
        "id": "jCKGo8v6DR_p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### COVERAGE"
      ],
      "metadata": {
        "id": "VusiOdyTdlcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Coverage Is from OneDrive so we downloaded it using it's curl command and then we extracted it's content.***"
      ],
      "metadata": {
        "id": "6QRP6duTfIPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://storage.live.com/downloadfiles/V1/Zip?authKey=%21ADJSupKlX%5FIj8Yc&application=1141147648' \\\n",
        "  -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7' \\\n",
        "  -H 'Accept-Language: en-US,en;q=0.9' \\\n",
        "  -H 'Cache-Control: max-age=0' \\\n",
        "  -H 'Connection: keep-alive' \\\n",
        "  -H 'Content-Type: application/x-www-form-urlencoded' \\\n",
        "  -H 'Cookie: wlidperf=FR=L&ST=1701337549707; PPLState=1; mkt=en-US; MSPAuth=Disabled; MSPProf=Disabled; NAP=V=1.9&E=1cc3&C=RuGrWk0suTTy6_xP7TiKMuobS4WTJxL6uzp0VQ0IkiHPY8AyMo7c-A&W=1; ANON=A=05C361638D7B64E69AB0999DFFFFFFFF&E=1d1d&W=1; MUID=10FDC95B1BF166B03C2BDA3B1A8667F1; WLSSC=EgArAgMAAAAMgAAAEAABCLNeLPfpwj0mB4EH5zgnyvPE/y9qbT1BZeqIgbUGlCxeMw3154ufCY3Gd+QiUDWOnAOeSgw/CwpqEedi0lX30uhYuFswQzLfVvHxSh8IeF+YF1fNDYLL+AuE6x1oSNnbLJ/iyqMopEIO1ec1PoKVvyHSG26LCwAIBEE/K3rQVuUOvlul4Mp1oAiePk6PGVyjdIwH8ZT6ML6z3YoSMHIm6vMM1W6+igJ4CuEAKqTzOQ8UaceIFaZJHY9v52k+cLK0Oy2ze70DZ6cNQnnbGzHbEzuQMH8bf3XivPnc+sXc3QF4g9iCcTxNFdFBMF4pm/ZpgjwZPusUQg0UJMfY1551LxoBfgAaAf5/AwD9w/o1vOB1Zc5ZaGUQJwAAChCgABAXAGdoYXppLmpyaWJpQG91dGxvb2suZnIAWwAAI2doYXppLmpyaWJpJW91dGxvb2suZnJAcGFzc3BvcnQuY29tAAABzFROAAAAAAAABAwCAACNY1VAAAZDAAVHaGF6aQAFSnJpYmkAAAAAAAAAAAAAAAAAAAAAAACV1PZXL5uzrQAAvOB1Zc4A32UAAAAAAAAAAAAAAAAQADE5Ni4yMzQuMjQ2LjE0MwAFAwAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAATCH7am1G/IsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAAAA; xid=e8e465b1-e55f-46ab-891b-980b697fc747&lk_HC&ODSP-ODWEB-ODCF&394; SAToken0=; SAToken1=; wla42=ZHNtMDFwYXAwMDkqMSwyRjlCQjNBRDk1RDRGNjU3LDAsLDAsLTEsLTE=; E=P:KE7pv5n524g=:ffOf3wi2QLDSkwi2y2xNRzkdBXrtrX8GqBr4zDAA/7U=:F; xidseq=13' \\\n",
        "  -H 'Origin: https://onedrive.live.com' \\\n",
        "  -H 'Sec-Fetch-Dest: iframe' \\\n",
        "  -H 'Sec-Fetch-Mode: navigate' \\\n",
        "  -H 'Sec-Fetch-Site: same-site' \\\n",
        "  -H 'Upgrade-Insecure-Requests: 1' \\\n",
        "  -H 'User-Agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36' \\\n",
        "  -H 'sec-ch-ua: \"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"' \\\n",
        "  -H 'sec-ch-ua-mobile: ?1' \\\n",
        "  -H 'sec-ch-ua-platform: \"Android\"' \\\n",
        "  --data-raw 'resids=4B518F0277851508%21709&authKey=%21ADJSupKlX_Ij8Yc' \\\n",
        "  --compressed \\\n",
        "  --output \"coverage.zip\""
      ],
      "metadata": {
        "id": "sNELcmZResKl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fb349734-aab0-4938-959e-67ece079b288"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  102M    0  102M    0    56   625k      0 --:--:--  0:02:48 --:--:--  113k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/coverage/\"\n",
        "if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "!unzip -qq \"/content/coverage.zip\" -d \"$path\""
      ],
      "metadata": {
        "id": "MPczOAsDdoEx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We have donwloaded all the dataset except for NIST2016, because we couldn't find any trace of that dataset in the provided link***"
      ],
      "metadata": {
        "id": "Eu1gcnTk2BBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = \"#ffd700\" size = 6> Code Inspection and Performance Assessment\n",
        "We will Examine now and elucidate the implementation of the code. We will Assess the code from various angles, encompassing its implementation simplicity, performance attributes, and its adaptability for future development."
      ],
      "metadata": {
        "id": "WiiHJkr1FPKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A crucial aspect of ensuring research reproducibility involves evaluating the clarity and efficiency of the authors' code documentation. The objective of this section is to assess whether the code is not only easy to implement but also optimized for execution in less powerful environments."
      ],
      "metadata": {
        "id": "rDAS7YuMHnve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Clone Author's code`"
      ],
      "metadata": {
        "id": "ILTY0C4tUEOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't be directly cloning the author's code from their repository due to modifications we've implemented. Instead, we forked the repository, applied the necessary changes, and will clone our dedicated repository for this competition. In this section, we'll provide guidance on the applied modifications to facilitate the implementation of the author's code in our Colab environment."
      ],
      "metadata": {
        "id": "L5riwE9KV6_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Ghazi-jr/CS-challenge.git"
      ],
      "metadata": {
        "id": "B1jpYfhPUDyb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9a57f51f-fa0d-456a-8c30-a930a76ae4b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CIS-challenge'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 76 (delta 14), reused 71 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (76/76), 54.83 KiB | 3.92 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We will download too the paths folder, which contains the text files that defines our different train, test, and validation image paths***"
      ],
      "metadata": {
        "id": "XB1Ui1RNXIFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "link = \"https://www.dropbox.com/s/opjpz9hoy5xm4um/paths.zip?dl=0\"\n",
        "out_name = \"paths.zip\"\n",
        "out_path = \"paths\"\n",
        "\n",
        "download_dataset_dropbox(link, out_name, out_path)"
      ],
      "metadata": {
        "id": "XBdbLhaUU_aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We found out also that the PreciseRolPooling extention isn't updated in the author's code and this will cause some errors in the training of our models so we will clone the Latest Version Of PreciseRolPooling To fix the deprecated implementation of THC/THC.h**"
      ],
      "metadata": {
        "id": "5XuQYg7YYk2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Correcting THC/THC.h deprecation\n",
        "\n",
        "!git clone https://github.com/vacancy/PreciseRoIPooling.git\n",
        "!rm -rf /content/CIS-challenge/utils/lib/nn/prroi_pool\n",
        "!cp -r /content/PreciseRoIPooling/pytorch/prroi_pool /content/CIS-challenge/utils/lib/nn/\n",
        "\n",
        "!rm -rf /content/CIS-challenge/utils/lib/nn/prroi_pool/src/prroi_pooling_gpu_impl.cu\n",
        "!rm -rf /content/CIS-challenge/utils/lib/nn/prroi_pool/src/prroi_pooling_gpu_impl.cuh\n",
        "\n",
        "!cp /content/PreciseRoIPooling/src/prroi_pooling_gpu_impl.cu /content/CIS-challenge/utils/lib/nn/prroi_pool/src/\n",
        "!cp /content/PreciseRoIPooling/src/prroi_pooling_gpu_impl.cuh /content/CIS-challenge/utils/lib/nn/prroi_pool/src/"
      ],
      "metadata": {
        "id": "tKiarjQbYlv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The files downloaded from the given path contain mappings to images specific to the author's environment. Hence, it is necessary to parse these paths and modify them to align with our environment. Those files are crucial for the training and evaluation of the models.***\n",
        "<br>\n",
        "***Again for the purpose of this report we will not include the code of this part since it's quite long but the notebook will contain the detailled implementation of this process***"
      ],
      "metadata": {
        "id": "QRlGxPB-07Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Defining New Path Mappings\n",
        "\n",
        "#Old CASIAv2 Mapping\n",
        "old_casia2_au_image_path = \"[ROOT]/ori/CASIAv2//images/Au\"\n",
        "old_casia2_tp_image_path = \"[ROOT]/ori/CASIAv2//images/Tp\"\n",
        "old_casia2_tp_masks_path = \"[ROOT]/ori/CASIAv2//masks\"\n",
        "old_casia2_tp_edges_path = \"[ROOT]/ori/CASIAv2//edges\"\n",
        "#New CASIAv2 Mapping\n",
        "new_casia2_au_image_path = \"/content/casiav2/CASIA2.0_revised/Au\"\n",
        "new_casia2_tp_image_path = \"/content/casiav2/CASIA2.0_revised/Tp\"\n",
        "new_casia2_tp_masks_path = \"/content/casiav2/CASIA2.0_revised/CASIA2.0_Groundtruth\"\n",
        "new_casia2_tp_edges_path = \"/content/casiav2/CASIA2.0_revised/CASIA2.0_Groundtruth\"\n",
        "\n",
        "#Old CASIAv1 Mapping\n",
        "old_casia1_au_image_path = \"[ROOT]/ori/CASIAv1//images/Au\"\n",
        "old_casia1_tp_image_path = \"[ROOT]/ori/CASIAv1//images/Modified_Tp\"\n",
        "old_casia1_tp_masks_path = \"[ROOT]/ori/CASIAv1//masks\"\n",
        "old_casia1_tp_edges_path = \"[ROOT]/ori/CASIAv1//edges\"\n",
        "#New CASIAv1 Mapping\n",
        "new_casia1_au_image_path = \"/content/casiav1/CASIA_1.0_dataset/Au\"\n",
        "new_casia1_tp_image_path = \"/content/casiav1/CASIA_1.0_dataset/Tp\"\n",
        "new_casia1_tp_masks_path = \"/content/casiav1/CASIA_1.0_dataset/CASIA_1.0_groundtruth\"\n",
        "new_casia1_tp_edges_path = \"/content/casiav1/CASIA_1.0_dataset/CASIA_1.0_groundtruth\"\n",
        "\n",
        "#Old Columbia Mapping\n",
        "old_columbia_au_image_path = \"[ROOT]/ori/Columbia//4cam_auth\"\n",
        "old_columbia_tp_image_path = \"[ROOT]/ori/Columbia//4cam_splc\"\n",
        "old_columbia_tp_masks_path = \"[ROOT]/ori/Columbia//masks\"\n",
        "old_columbia_tp_edges_path = \"[ROOT]/ori/Columbia//edges\"\n",
        "#New Columbia Mapping\n",
        "new_columbia_au_image_path = \"/content/colombia/4cam_auth\"\n",
        "new_columbia_tp_image_path = \"/content/colombia/4cam_splc\"\n",
        "new_columbia_tp_masks_path = \"/content/colombia/masks/edgemask\"\n",
        "new_columbia_tp_edges_path = \"/content/colombia/masks/edgemask\"\n",
        "\n",
        "#Old COVERAGE Mapping\n",
        "old_coverage_au_image_path = \"[ROOT]/ori/COVERAGE//image\"\n",
        "old_coverage_tp_image_path = \"[ROOT]/ori/COVERAGE//image\"\n",
        "old_coverage_tp_masks_path = \"[ROOT]/ori/COVERAGE//mask\"\n",
        "old_coverage_tp_edges_path = \"[ROOT]/ori/COVERAGE//edge\"\n",
        "\n",
        "#New COVERAGE Mapping\n",
        "new_coverage_au_image_path = \"/content/coverage/image\"\n",
        "new_coverage_tp_image_path = \"/content/coverage/image\"\n",
        "new_coverage_tp_masks_path = \"/content/coverage/mask\"\n",
        "new_coverage_tp_edges_path = \"/content/coverage/mask\"\n",
        "\n",
        "#DeepFake Mapping\n",
        "old_deep_path = \"[ROOT]\"\n",
        "new_deep_path = \"/content/deep-data\"\n",
        "\n",
        "#NIST\n",
        "to_remove = \"NIST2016\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "bprvEpDFhbgn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Parsing Paths\n",
        "\n",
        "folder_path = '/content/paths/'\n",
        "\n",
        "def parse_deep_paths(file_path, old_path, new_path) :\n",
        "\n",
        "    file_path = file_path.replace(old_path, new_path)\n",
        "\n",
        "    return file_path\n",
        "\n",
        "def parse_paths(\n",
        "        file_path,\n",
        "        old_au_path,\n",
        "        old_tp_path,\n",
        "        old_mask_path,\n",
        "        old_edge_path,\n",
        "        new_au_path,\n",
        "        new_tp_path,\n",
        "        new_mask_path,\n",
        "        new_edge_path\n",
        "    ) :\n",
        "\n",
        "    file_path = file_path.replace(old_au_path, new_au_path)\n",
        "    file_path = file_path.replace(old_tp_path, new_tp_path)\n",
        "    file_path = file_path.replace(old_mask_path, new_mask_path)\n",
        "    file_path = file_path.replace(old_edge_path, new_edge_path)\n",
        "\n",
        "    return file_path\n",
        "\n",
        "def modify_line(line, file_name):\n",
        "    if \"CASIAv2\" in line :\n",
        "        line = parse_paths(\n",
        "            line,\n",
        "            old_casia2_au_image_path,\n",
        "            old_casia2_tp_image_path,\n",
        "            old_casia2_tp_masks_path,\n",
        "            old_casia2_tp_edges_path,\n",
        "            new_casia2_au_image_path,\n",
        "            new_casia2_tp_image_path,\n",
        "            new_casia2_tp_masks_path,\n",
        "            new_casia2_tp_edges_path\n",
        "        )\n",
        "\n",
        "    elif \"CASIAv1\" in line :\n",
        "        line = parse_paths(\n",
        "            line,\n",
        "            old_casia1_au_image_path,\n",
        "            old_casia1_tp_image_path,\n",
        "            old_casia1_tp_masks_path,\n",
        "            old_casia1_tp_edges_path,\n",
        "            new_casia1_au_image_path,\n",
        "            new_casia1_tp_image_path,\n",
        "            new_casia1_tp_masks_path,\n",
        "            new_casia1_tp_edges_path\n",
        "        )\n",
        "    elif \"Columbia\" in line :\n",
        "        line = parse_paths(\n",
        "            line,\n",
        "            old_columbia_au_image_path,\n",
        "            old_columbia_tp_image_path,\n",
        "            old_columbia_tp_masks_path,\n",
        "            old_columbia_tp_edges_path,\n",
        "            new_columbia_au_image_path,\n",
        "            new_columbia_tp_image_path,\n",
        "            new_columbia_tp_masks_path,\n",
        "            new_columbia_tp_edges_path\n",
        "        )\n",
        "    elif \"COVERAGE\" in line :\n",
        "        line = parse_paths(\n",
        "            line,\n",
        "            old_coverage_au_image_path,\n",
        "            old_coverage_tp_image_path,\n",
        "            old_coverage_tp_masks_path,\n",
        "            old_coverage_tp_edges_path,\n",
        "            new_coverage_au_image_path,\n",
        "            new_coverage_tp_image_path,\n",
        "            new_coverage_tp_masks_path,\n",
        "            new_coverage_tp_edges_path\n",
        "        )\n",
        "    elif \"NIST2016\" in line :\n",
        "        line = \"\"\n",
        "    elif not \"sf\" in file_name :\n",
        "        line = parse_deep_paths(line, old_deep_path, new_deep_path)\n",
        "    else :\n",
        "        pass\n",
        "\n",
        "    return line\n",
        "\n",
        "file_list = os.listdir(folder_path)\n",
        "for file_name in file_list:\n",
        "\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    if os.path.isfile(file_path):\n",
        "        print(f\"Processing file: {file_name}\")\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        modified_lines = [modify_line(line, file_name) for line in lines]\n",
        "        with open(file_path, 'w') as file:\n",
        "            file.writelines(modified_lines)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "x4gGXVPGvvX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***In the remaining of the notebook we will switch to work on the CIS-challenge folder***"
      ],
      "metadata": {
        "id": "OJkuA07-06Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CS-challenge/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ump5eq2vlf6r",
        "outputId": "9f5e07ec-8a39-47f4-c2fc-30a3b0ad63cf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CIS-challenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Code Inspecting And Assessment`\n",
        "During this phase, we delved into scrutinizing the author's code, exploring its functionality and assessing its suitability for execution in an environment like Google Colab.\n",
        "\n",
        "Several key observations and conclusions emerged from our examination, shedding light on the code's accessibility for reproducing the authors' research:\n",
        "\n",
        " - Code Readability: The code demonstrated a commendable level of readability, facilitating comprehension. However, we identified room for improvement in terms of documentation and comments. More extensive explanatory notes would enhance understanding of the code's structure and, crucially, the construction of the models. This additional information would be instrumental for anyone seeking to build upon the existing models or experiment with different architectures.\n",
        "\n",
        " - Code Performance: Our evaluation extended to the code's performance characteristics, particularly its adaptability to less powerful computing environments. While research settings often boast ample resources, ensuring the code's viability in more modest setups is vital for broader accessibility and reproducibility. Overall, we managed to successfully replicate most of the authors' experiments, yet encountered challenges in certain cases, as elaborated in the Experimenting Section. As part of our optimization considerations, we recognized the authors' adherence to best practices, such as employing multiple workers in their dataloader (Dataloader(dataset, num_workers=4*num_GPU)), utilizing pin_memory (Dataloader(dataset, pin_memory=True)), and implementing DistributedDataParallel. Although various other optimization techniques were utilized, we specifically highlight those related to memory management, a critical aspect, especially in Colab.\n",
        "\n",
        " - Update: Notably, the only issue we encountered in the author's code pertained to the PreciseRoIPooling module. We addressed this by updating the code, recognizing that the original implementation had been modified to resolve concerns related to the deprecated THC/THC.h library.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JYNYd7GiHFOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = \"#ffd700\" size = 6> Experiments (Author's Work)\n",
        "\n",
        "- Conduct the Authour's experiments to validate the proposed solution.\n",
        "- Apply the solution to various datasets for localization in manipulated areas.\n",
        "- Present results, visualizations, and comparisons with existing methods."
      ],
      "metadata": {
        "id": "k5NLrmD4T1p6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***At this step we were ready to run the authors experiments, In total the author's conducted 9 experiment We will provide in this report the code executed to run the experiments along side the problem we faced to implement that***"
      ],
      "metadata": {
        "id": "OGINaqCx6ta3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The author's used a cluster of 4 powerfull GPUs to run those nine experiments on 50 epochs in a distrubuted manner. However since we didn't have those resources we created a Human cluster. So we created a shared drive to host the results of each experiment and we divided those experiments between us to optimize the running time due to the limitations of using google colab"
      ],
      "metadata": {
        "id": "bQcFX5SV-8mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Human Cluster Experiment :)\n",
        "DRIVE_PATH = \"/content/drive/\"\n",
        "USER = \"/Shareddrives/CS_CHALLENGE/Experiments/Ghazi/\"\n",
        "\n",
        "drive_checkpoint_path = DRIVE_PATH + USER\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(DRIVE_PATH, force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay_ijiSMGCen",
        "outputId": "6dd0971c-14e7-4a1b-decd-bd5d7cc6aae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have access to only one Tesla T4 on Colab, we will disable distributed computing. Consequently, we will adjust the values of `LOCAL_RANK`, `RANK`, and `WORLD_SIZE` in our environment."
      ],
      "metadata": {
        "id": "Lfo1nuGYY8u6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Experiments"
      ],
      "metadata": {
        "id": "G2KyjXA9wx0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"8888\""
      ],
      "metadata": {
        "id": "GAcicP5ZtSVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 1"
      ],
      "metadata": {
        "id": "hCiDh4-p_0wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Params for experiment 1\n",
        "train_path_file = \"../paths/paths_train_sf.txt\"\n",
        "validation_path_file = \"../paths/paths_val_sf.txt\"\n",
        "model = \"mvssnet\"\n",
        "image_size = 224\n",
        "n_epochs = 50\n",
        "optim = \"adam\""
      ],
      "metadata": {
        "id": "_t33YCr24iS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u train_torch.py --paths_file \"$train_path_file\" --val_paths_file \"$validation_path_file\" --model \"$model\" --image_size \"$image_size\" --n_epochs \"$n_epochs\" --optim \"$optim\""
      ],
      "metadata": {
        "id": "iQnL-m-WE8am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***For this part the checkpoint_name and epoch_model_name are a place holders for each team member to save his experiment in the drive and we run the evaluation script on the best recorded epoch***"
      ],
      "metadata": {
        "id": "C-mQuZb4_iTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Params\n",
        "test_path_file = \"../paths/paths_test_sf.txt\"\n",
        "\n",
        "checkpoint_name = \"\" #Update this according to experiment\n",
        "epoch_model_name = \"\" #Update this according to best epoch\n",
        "\n",
        "checkpoint_path = f\"/content/CIS-challenge/checkpoints/{checkpoint_name}/{epoch_model_name}\""
      ],
      "metadata": {
        "id": "xp6Yl7LvR_Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u evaluate.py --paths_file \"$test_path_file\" --load_path \"$checkpoint_path\" --model \"$model\" --image_size \"$image_size\""
      ],
      "metadata": {
        "id": "Hx70RQfbLPjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Persist Checkpoint to drive\n",
        "checkpoint_path = f\"/content/CIS-challenge/checkpoints/{checkpoint_name}\"\n",
        "#Copy Out folder to checkpoint\n",
        "out_path = \"/content/CIS-challenge/out\"\n",
        "\n",
        "!cp -r \"$out_path\" \"$checkpoint_path\"\n",
        "!cp -r \"$checkpoint_path\" \"$drive_checkpoint_path\""
      ],
      "metadata": {
        "id": "LVQ1Ky4IIgmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We decided to include only the boiler plate to run one experiment and you could find always in the notebook the full experiment's code***"
      ],
      "metadata": {
        "id": "FAgq96FMAEpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Conclusion*** :<br>\n",
        "Broadly, we successfully executed the majority of experiments employing the authors' specified hyperparameters. However, due to memory constraints and to avert Colab crashes, we had to diminish the image size to 224 from the original 512. Upon running the evaluation script, the outcomes were closely aligned, and the masks generated by different models exhibited significant variations in quality. It is important to acknowledge some deviations in the results, as the training did not precisely utilize the full image quality."
      ],
      "metadata": {
        "id": "ET2EJdc3AMgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helpers For Assessment"
      ],
      "metadata": {
        "id": "mmYs6Dy1w5WE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***In this section, we aimed to incorporate the foundational code as it constituted the core of our evaluation process. Detailed results will be elaborated upon in the presentation; however, the following code snippet underscores our approach in obtaining experiments from the shared drive. Additionally, we downloaded the authors' experiments from the provided links, enabling us to execute the evaluation script and scrutinize any discernible differences.***"
      ],
      "metadata": {
        "id": "spmo1JHnA9wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_from_drive(drive_path) :\n",
        "  models_path = \"/content/models/\"\n",
        "  if not os.path.exists(models_path):\n",
        "      os.makedirs(path)\n",
        "      print(f\"Models Path Created !\")\n",
        "\n",
        "  !cp -r \"$drive_path\" \"$models_path\"\n",
        "\n",
        "\n",
        "def download_auth_model(link, file_name, save_path) :\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    full_path = f\"{save_path}{file_name}\"\n",
        "    !wget -O  \"$full_path\" \"$link\"\n",
        "\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/\"\n",
        "USER = \"/Shareddrives/CS_CHALLENGE/Experiments/Ghazi/\"\n",
        "drive_checkpoint_path = DRIVE_PATH + USER\n",
        "\n",
        "checkpoint_name = \"\"\n",
        "epoch_model_name = \"\""
      ],
      "metadata": {
        "id": "x_YrRgMG0aGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = drive_checkpoint_path + checkpoint_name + \"/\" + epoch_model_name\n",
        "load_model_from_drive(drive_path)"
      ],
      "metadata": {
        "id": "yA5iOswB18Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_auth_model(\"https://www.dropbox.com/s/zzk4eump5xfbqmz/9.pth?dl=0\", \"experiment_9.pth\", \"/content/models/\")"
      ],
      "metadata": {
        "id": "fYI3uqoRMa1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path_file = \"../paths/paths_test.txt\"\n",
        "checkpoint_path = \"/content/models/experiment_9.pth\"\n",
        "\n",
        "!python -u evaluate.py --paths_file \"$test_path_file\" --load_path \"$checkpoint_path\" --model \"ours\" --image_size 512"
      ],
      "metadata": {
        "id": "7nj9S6oeZAyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = \"#ffd700\" size = 6> Optimization (Our's Work)\n",
        "- Explore opportunities for solution optimization.\n",
        "- Apply various optimization techniques to achieve better results."
      ],
      "metadata": {
        "id": "YWMQtSKHw7eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the optimization phase, our approach extended beyond conventional methods. Initially, we evaluated the performance of the authors' code, finding no immediate optimization opportunities, given the robust performance of their architecture and model. While contemplating the implementation of an alternative model, we recognized the challenge of surpassing the efficacy of the state-of-the-art approach developed by the authors.\n",
        "\n",
        "Prompted by curiosity, we questioned the authors' decision not to segregate DeepFake data from shallowFake data. The rationale became apparent: they aimed to create a model with strong generalization capabilities across both datasets. However, upon scrutinizing their results, we observed that training the model on individual datasets yielded superior outcomes. This led us to a strategic decision – implementing a classification model initially to distinguish between shallow fake and deepfake data. This approach holds real-world significance, considering the unknown nature of incoming images. By leveraging the classifier, we could discern the nature of each instance of fakery and subsequently employ segmentation models trained specifically on either the shallowFake or deepFake datasets."
      ],
      "metadata": {
        "id": "43ZDo4FWCGJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Classifier Results\n",
        "\n",
        "%time\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import feature\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "possible_sf_paths = [\"/content/casiav2/CASIA2.0_revised/Tp\", \"/content/casiav1/CASIA_1.0_dataset/Tp/CM\", \"/content/casiav1/CASIA_1.0_dataset/Tp/Sp\", \"/content/colombia/4cam_splc\"]\n",
        "possible_df_paths = [\"/content/deep-data/image/Deepfakes\", \"/content/deep-data/image/Face2Face\", \"/content/deep-data/image/FaceSwap\", \"/content/deep-data/image/NeuralTextures\", \"/content/deep-data/image/actors\"]\n",
        "possible_au_paths = [\"/content/casiav1/CASIA_1.0_dataset/Au\", \"/content/casiav2/CASIA2.0_revised/Au\", \"/content/colombia/4cam_auth\", \"/content/deep-data/image/youtube\"]\n",
        "\n",
        "def extract_noise_patterns(image):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    laplacian = cv2.Laplacian(gray_image, cv2.CV_64F)\n",
        "    laplacian_var = laplacian.var()\n",
        "    return [laplacian_var]\n",
        "\n",
        "def extract_compression_artifacts(image):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    dct_coeffs = cv2.dct(np.float32(gray_image))\n",
        "    compression_features = dct_coeffs[8:16, 8:16].flatten()\n",
        "    return compression_features.tolist()\n",
        "\n",
        "def extract_color_histograms(image):\n",
        "    hist_b = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
        "    hist_g = cv2.calcHist([image], [1], None, [256], [0, 256])\n",
        "    hist_r = cv2.calcHist([image], [2], None, [256], [0, 256])\n",
        "\n",
        "    color_histogram_features = np.concatenate([hist_b, hist_g, hist_r]).flatten()\n",
        "    return color_histogram_features.tolist()\n",
        "\n",
        "def extract_splicing_artifacts(image):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    lbp = feature.local_binary_pattern(gray_image, P=8, R=1, method=\"uniform\")\n",
        "    splicing_features = np.histogram(lbp, bins=np.arange(0, 10), density=True)[0]\n",
        "    return splicing_features.tolist()\n",
        "\n",
        "def extract_blurring_and_sharpness(image):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    laplacian_var = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
        "    return [laplacian_var]\n",
        "\n",
        "def load_images_and_labels(folder_path, label):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in tqdm(os.listdir(folder_path)):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png', '.tif')):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            noise_features = extract_noise_patterns(img)\n",
        "            compression_features = extract_compression_artifacts(img)\n",
        "            color_histogram_features = extract_color_histograms(img)\n",
        "            splicing_features = extract_splicing_artifacts(img)\n",
        "            blurring_sharpness_features = extract_blurring_and_sharpness(img)\n",
        "\n",
        "            img_features = (\n",
        "                noise_features +\n",
        "                compression_features +\n",
        "                color_histogram_features +\n",
        "                splicing_features +\n",
        "                blurring_sharpness_features\n",
        "            )\n",
        "\n",
        "            features.append(img_features)\n",
        "            labels.append(label)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "deepfake_folder = possible_df_paths[0]\n",
        "shallowfake_folder = possible_sf_paths[0]\n",
        "\n",
        "deepfake_features, deepfake_labels = load_images_and_labels(deepfake_folder, label='deepfake')\n",
        "shallowfake_features, shallowfake_labels = load_images_and_labels(shallowfake_folder, label='shallowfake')\n",
        "\n",
        "for p in possible_df_paths[1:] :\n",
        "  df_f, df_l = load_images_and_labels(p, label='deepfake')\n",
        "  deepfake_features.extend(df_f)\n",
        "  deepfake_labels.extend(df_l)\n",
        "\n",
        "for p in possible_sf_paths[1:] :\n",
        "  sf_f, sf_l = load_images_and_labels(p, label='shallowfake')\n",
        "  shallowfake_features.extend(sf_f)\n",
        "  shallowfake_labels.extend(sf_l)\n",
        "\n",
        "\n",
        "all_features = deepfake_features + shallowfake_features\n",
        "all_labels = deepfake_labels + shallowfake_labels\n",
        "\n",
        "X = np.array(all_features)\n",
        "y = np.array(all_labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print('Classification Report:')\n",
        "print(classification_rep)\n",
        "\n",
        "# save classifier\n",
        "with open('classifier.pkl','wb') as f:\n",
        "    pickle.dump(clf,f)"
      ],
      "metadata": {
        "id": "oOD1CVdxMtLv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "outputId": "87bc8676-08d5-4a4a-ff9c-10a58711c461"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 0 ns, sys: 4 µs, total: 4 µs\n",
            "Wall time: 7.39 µs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [06:47<00:00,  4.91it/s]\n",
            "100%|██████████| 5123/5123 [05:24<00:00, 15.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.99\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    deepfake       0.99      0.99      0.99       424\n",
            " shallowfake       1.00      1.00      1.00      1001\n",
            "\n",
            "    accuracy                           0.99      1425\n",
            "   macro avg       0.99      0.99      0.99      1425\n",
            "weighted avg       0.99      0.99      0.99      1425\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the classifier training phase, we extracted relevant features from our model. Following thorough research, we identified distinctive characteristics enabling the differentiation between deep fake and shallow fake images, including:\n",
        "\n",
        "- Noise patterns\n",
        "- Compression artifacts\n",
        "- Color histograms\n",
        "- Splicing artifacts\n",
        "- Blurring and sharpness\n",
        "\n",
        "Leveraging these features, a RandomForestModel achieved impressive scores, attaining a 99% accuracy. This outcome instills confidence in the classifier's ability to accurately categorize images. Consequently, we are poised to integrate this classification into the ensemble script, where the segmented models can be employed with assurance."
      ],
      "metadata": {
        "id": "6naq5_akDouE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_path_file = \"../paths/paths_test.txt\"\n",
        "path_sf = \"\" #We should add here the path to our best performing model trained on SF data only\n",
        "path_df = \"\" #We should add here the path to our best performing model trained on DF data only"
      ],
      "metadata": {
        "id": "bspxlXYBEwil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u ensembler.py --paths_file \"$test_path_file\" --load_path_sf \"$path_sf\" --load_path_df \"$path_df\" --model \"ours\" --image_size 224"
      ],
      "metadata": {
        "id": "u4NAo0wwEqIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***In conclusion, armed with a robust classifier, we are now positioned to optimize each segmentation model independently. This alleviates concerns about introducing bias from other datasets, fostering the prospect of developing a cutting-edge image fakery detection model.***"
      ],
      "metadata": {
        "id": "71rurthGFI9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = \"#ffd700\" size = 6> Buisness Model\n",
        "\n",
        "Given our exceptional approach to detecting image fakery, we are contemplating a startup idea which we aim to succinctly outline through the Business Model Canvas (BMC)."
      ],
      "metadata": {
        "id": "kAqMny_tFlMH"
      }
    }
  ]
}